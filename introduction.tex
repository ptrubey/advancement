
\section{Introduction}
Atmospheric rivers are temporary events, where large elongated regions of high concentrations of
  water vapor are developed in the atmosphere and carry huge amounts of water potentially thousands
  of miles.  The amount of water in transit during these events dwarfs that of terrestrial rivers.
  For the targetted region, the atmospheric river can represent a significant portion of the
  precipitation the region will experience.  Such events are thus of great interest to meterologists,
  as well as farmers, \makenote{expand and cite}.

One metric by which we might identify and declare atmospheric rivers is the integrated water vapor
  transport, or \emph{IVT} \bruno{Why does `` emph '' produce an underlying word instead of italics?
  It messes up the style of the references}\makenote{I don't know.  It doesn't when I build it on my
  computer.  This is only happening in overleaf...}.  This value represents the total amount of water
  vapor being transported in an atmospheric column--that is, a column of the troposphere of particular
  size. These values can be measured by dropsondes, but the values we are using are estimated as part
  of a data product\makenote{needs citation}.  An observation from these data includes a reading (estimated
  or measured) at grid cell at a period in time; where a grid cell represents the surface of the
  earth associated with the atmospheric column.  Our specific data comes from the coast of California,
  with daily readings of IVT covering 30 years, omitting leap days.

We have two such datasets, in differing spatial resolutions.  The lower resolution splits the coast
  of california into 8 grid cells, while the higher resolution does so into 46 grid cells.
  \bruno{
  At this point you have to say what is the problem with these data that is relevant for this work.
  I suppose the story could something like: we are interested is extreme values of IVT, and in how
  likely it is that an extreme IVT will happen jointly in more than one location, as well as in
  identifying configurations of the IVT values at the different location that may be anomalous.
  The you say that you will propose different models and that you will evaluate them using
  the two different datasets.
  }
  We will be looking at relative model performance on these two datasets as a means of evaluating
  how well any model we propose scales.  We are specifically interested in extremal dependence--the
  relationship between the upper tails of dimensions of the distribution.  In this case, that means
  the relationship between extreme values in different grid cells.  We are going to be looking at
  point-in-time behavior rather than considering the time series nature of the data--that
  relationship may come later.

As we are interested in the extremal dependence, it makes sense that we would choose to represent
  this data using tools from extreme value theory.  Extreme value theory, or \emph{EVT}, seeks to
  model and assess probability of observing extreme events.  Such a topic is applicable generally,
  but it finds particularly strong use among such fields as finance \citep{allen2013},
  climatology \citep{trepanier2018}, and insurance \citep{beirlant1994}.  In these fields, extreme
  events may represent significant loss to the body commissioning the study.  For instance, an
  insurance company might commission a study on extreme weather events, as an extreme weather event
  localized to a particular region could cause a spike in claims from that region.  Extreme value
  theory offers us a tool set for making inference about the tails of a distribution, without having
  observed said tails.  For
  instance, with an extreme weather event like flooding, we can make predictions about return
  levels--the average time until an observation of a particular magnitude occurs--without having seen
  an observation of that magnitude, or having observed that long.  In the context of our motivating
  example, extreme values in the IVT may represent the formation of an atmospheric river, which has
  dramatic effects on precipitation and ground water.  The formation of an IVT may lead to flooding
  and other negative effects, but is also contributes necessary water for irrigation and agriculture.
  Development of statistical tools regarding these extreme events becomes necessary for making informed
  decisions.

\subsection{Univariate EVT - Maxima}
Regarding the asymptotic behavior of extreme events, there are a couple major strategies for conducting
  inference--developing probabilistic estimates of extreme behavior.  First developed is the theory
  of a limiting distribution on maxima--the largest observation from a sample.  For a sample ${\bf x}$
  where ${\bf x} = (x_1,\ldots,x_n)$ represents a sequence of $n$ independent random variables from a
  distribution function $F$, the distribution of the maximum $M_n$ of this sequence can be derived
  as:
  \begin{equation*}
    \begin{aligned}
      \text{Pr}(M_n\leq z) &= \text{Pr}(X_1 \leq z, \ldots, X_n \leq z)\\
        &= \text{Pr}(X_1\leq z)\times\ldots\times\text{Pr}(X_n\leq z)\\
        &= F(z)^n.
    \end{aligned}
  \end{equation*}
  In situations where $F$ is unknown, we seek to approximate the behavior of $F^n$ as
  $n\rightarrow\infty$.  To ensure this does not degenerate to a point mass, we need to consider a
  standardized sequence of maxima. If this sequence stabilizes as $n$ increases, the a limiting
  distribution exists.  More specifically, if there exists some sequence of constants $a_n > 0$, $b_n$ s
  uch that:
  \begin{equation*}
    \text{Pr}\left[\frac{M_n - b_n}{a_n} \leq z\right] \stackrel{d}{\rightarrow} G(z)
  \end{equation*}
  as $n\rightarrow\infty$, then $G$ is a max-stable distribution, and $F$ is in the domain of
  attraction of that max stable distribution.  Maurice Fr{\'e}chet \cite{frechet1927} originates the
  field, identifying what would become known as the Fr{\'e}chet and Weibull distributions.
  \cite{weibull1951} expands the analysis of the Weibull distribution; the results
  of that work giving it its current name.  \cite{fisher1928} identify the Fr{\'e}chet
  and Weibull distributions, along with an as then unnamed third form, as the three limiting forms of
  the distribution of the maxima of a sample.  \cite{gumbel1935,gumbel1942} offers an analysis of that
  third form, now known as the Gumbel distribution.  Later works, including \cite{jenkinson1955}
  reparameterize all three forms as special cases of a single unifying form, the generalized extreme
  value distribution, \emph{GEV}:
  \begin{equation*}
    \label{eqn:gev}
    F(m \mid \mu, \sigma, \xi) = \exp\left\lbrace-\left[1 + \xi\left(\frac{x - \mu}{\sigma}\right)\right]^{-1/{\xi}}\right\rbrace.
  \end{equation*}
  Thus distributions in the domain of attraction of an EVD (Gumbel, Fr{\'e}chet, or Weibull) will be
  in the domain of attraction of the GEV. Now, one characteristic aspect of max stable distributions
  is that they feature the homogeneity property,
  \begin{equation*}
    \mu(tx) = \frac{1}{t}\mu(x).
  \end{equation*}
  Use of this property will be fundamental in the generalization of EVT to a multivariate setting.

As this distribution specifies asymptotic behavior for the maximum of a set of observations,
  inference assuming this distribution requires we specify some block of data that we would take the
  maximum in, and for the block report only that maximum.  A series of these blocks yielding a series
  of maxima allows us to conduct inference about the parameters of the distribution.  Taking only the
  maximum in a block of observations leads to the reduction of our sample size by a factor of
  $1/\text{block size}$. In problems where the data occur in natural blocks, such as an hourly time
  series where a natural block might be a day, this might be appropriate.  There is an implicit
  violation of the assumption of independence within a block, in most cases, but that violation is
  generally ignored. In data without a natural block, it may be difficult to justify an induced
  artitifial block.\makenote{need more better sentence.} Moreover, in general, retaining only one
  datum for each block increases the variability of the parameter estimates.

\subsection{Univariate EVT - Thresholding}
Another way we can conduct inference that is less wasteful of data, is to specify a threshold,
  and consider observations \emph{extreme} if they are in excess of that threshold.  From the
  start, if $F$ is in the domain of attraction of an EVD, then for a random variable $X\sim F$,
  exceedances over a large threshold $u$ will similarly have a limiting distribution, and that
  limiting distribution is in the Generalized Pareto family.  Again, let $X$ follow some distribution
  function $F$. It follows that:
  \begin{equation*}
    \text{Pr}\left[X > u + y\mid X > u\right] = \frac{1 - F(u + y)}{1 - F(u)}
  \end{equation*}
  for $y > 0$.  If $F$ is in the domain of attraction of an EVD, then
  $\lim\limits_{u\to\infty}\text{Pr}\left[X > u + y\mid X > u\right]$
  has a functional form--the survival function of a Pareto distribution. Let $X_1,\ldots,X_n$ be a
  sequence of random variables with the distribution function $F$.  Let $M_n = \max[x_1,\ldots,x_n]$.
  Suppose that $F$ is in the domain of attraction of the GEV, such that for large $n$,
  $\text{Pr}[M_n \leq z]\approx G(z)$.  Then, for large enough $u$, $\text{Pr}[X > u+y\mid x > u]$
  approximates to
  \begin{equation*}
    \label{eqn:gp}
    H(y) = 1 - \left(1 + \frac{\xi y}{\sigma}\right)^{-\frac{1}{\xi}}.
  \end{equation*}
  This defines the generalized Pareto family of distributions.  Thus, if block maxima have a
  limiting distribution $G$ within the EVD family, then threshold exceedances for a sufficiently
  high threshold have a limiting distribution $H$ within the Generalized Pareto (GP) family.  We can
  identify $\sigma$ as a scale parameter, but $\xi$ deserves special mention as the extremal index.
  We can interpret the Pareto tail probability as
  \begin{equation*}
    \lim\limits_{t\to\infty}\frac{1 - F(tx)}{1 - F(t)} = x^{-\frac{1}{\xi}}
  \end{equation*}
  for $x > 1$.  Importantly, $\xi$ from the limiting distribution of excesses over a threshold is the
  same parameter, and has the same value as the $\xi$ from the limiting distribution of the maximum
  observation of a sample.  Pickand\needcite and Hill\needcite provide estimators for this value
  relying on this result.

  \makenote{I need to add some sources for
  Pareto distribution, as well as further explanation.  minor explanation of the existence for
  various estimators beyond ML for chi as well.}

One other point of note is that for time series, the implicit assumption of independence for those
  observations in excess of a threshold is violated.  One means of dealing with this violation is
  to consider a string of observations in excess of a threshold as correlated, and only keep one
  of the string.\makenote{Need citation of paper that uses this approach...as we do as well.}

\subsection{Multivariate EVT - Maxima}
There is inherent difficulty in establishing what it means to be \emph{extreme} in multiple dimensions.
  \makenote{This needs rewriting}.  Multivariate EVT then splits into two components--establishing the
  marginal distributions, then estimating the dependence structure.  By establishing the marginal
  distributions, we can translate or standardize the margins to a common scale--allowing us to then
  estimate the dependence structure without having to deal with disparate scales.  This path of analysis
  is born out in the literature\needcite.


\subsection{Multivariate Threshold EVT}
\makenote{Rewrite is done to this point.}
\bruno{At this point you need to say that in this work we will focus on the methods based on
  threshold exceedance in a multivariate setting. You should introduce the topic (although you might
  have already done that in the introduction) indicating the difficulties that there exist in the
  generalization of a Pareto distribution in the multivariate case. Cite some papers, of course.
  You can then say that the traditional approach consists on focusing on the estimation of the
  joint tail behavior, separate from the marginal behavior. }

Assume that the $d$-dimensional vector ${\bf Z}$, after standardization, for some distribution $\mu$
  defined on the positive orthant of $\mathcal{R}^d$, then
  \begin{equation*}
    \lim_{n\to\infty} n\text{Pr}\left(\frac{1}{n}{\bf Z}\geq {\bf z}\right) = \mu\left([{\bf 0},{\bf z}]^c).
  \end{equation*}
  In this setting, $\mu$ corresponds to the asymptotic distribution of ${\bf Z}$ in extreme regions.
  In the terminology of EVT, this is known as the exponent measure.  This will feature the homogeneity
  property, such that $\mu(tA) = \frac{1}{t}\mu(A)$.  This implies that ${\bf Z}$ can be factorized
  as ${\bf Z} = R{\bf V}$, where $R = \pnorm{{\bf Z}}{\infty} > 1$, and
  ${\bf V} = {\bf Z}/\pnorm{{\bf Z}}{\infty} \in \mathcal{S}_{\infty}^{d-1}$.





Our motivating example lies in the domain of multivariate EVT, where we make inference about the
  distribution conditional on at least one threshold exceedance.  In multivariate EVT, the standard
  practice is to first standardize each dimension according to its marginal distribution, then
  estimate its dependence structure.\findcite  As has been stated, if each marginal distribution falls
  into the domain of attraction of the GEV, then threshold exceedances for a sufficiently
  \emph{sufficiently high} threshold will have a limiting distribution in the GP family\cite{beirlant2006}.
  Therefore, we estimate parameters for those marginal distributions considering only those
  observations that exceed the threshold.  For ${\bf x}$, for which the marginal distributions of its
  dimensions are within the domain of attraction of a GEV, standardization occurs as
  \begin{equation}
    Z_l = \left(1 + \xi_l\frac{X_l - b_{t,l}}{a_{t,l}}\right)_{+}^{1/\xi_l}.
  \end{equation}
  In practice, as estimation of these parameters is not the focus of this paper, we specify
  $b_{t,l} := \hat{F}_l^{-1}\left(1 - \frac{1}{l}\right)$, then fit the scale parameter $a_{t,l}$,
  and extremal index $\xi_l$ via maximum likelihood.  Note that $z_l > 1$ implies that $x_l > b_{t,l}$,
  meaning that the observation ${\bf x}$ is extreme in the $l$'th dimension.  Note also that
  $\sup_j Z_j$ follows a simple Pareto distribution.

\subsubsection{Limit Measures}
Multivariate EVT assumes the existence of a probability measure $\mu$ on ${\bf Z}$ such that
  \begin{equation}
    \lim_{n\rightarrow\infty}n\text{Pr}\left[\frac{1}{n}{\bf Z} \geq {\bf z}\right] = \mu\left([{\bf 0}, {\bf z}]^c\right).
    \label{eq:standard}
  \end{equation}
  $\mu$ is thus the asymptotic distribution of ${\bf Z}$ in extreme regions.  It exhibits the
  homogeneity property, such that for any region A, $\mu(rA) = r^{-1}\mu(A)$.  By this property we
  thus factorize ${\bf Z}$ into two components:
  \begin{equation}
    \begin{aligned}
      R &= \inorm{{\bf Z}} \in [1, \infty),\\
      {\bf V} &= \frac{{\bf Z}}{R} \in S_{\infty}^{d-1}.
    \end{aligned}
  \end{equation}
  That is, to say, we factorize ${\bf Z}$ into a radial component $R$, and an angular component,
  ${\bf V}$, which is the projection of ${\bf Z}$ onto the positive orthant of the $d$-dimensional
  unit hypersphere defined by the infinity norm, $S_{\infty}^{d-1}$. The radial component $R$, as we
  have stated, follows a simple pareto distribution, and as a consequence of the homogeneity property,
  is independent of the angular component ${\bf V}$.

As the angular component is now independent of the radial component, we can establish a distribution
  on solely the angular component.  For  $B\subset S_{\infty}^{d-1}$, We define the spectral
  (or angular) measure, $\Omega(B)$, as
  \begin{equation}
    \Omega(B) = \mu[{\bf z}: R({\bf z}) > 1, {\bf V} \in B].
  \end{equation}
  \bruno{This needs t be sharpened. I don't see why $R({\bf z})>1$ is needed. Also
  \[	\mu([0,1/\bm{z}]^c) = \int_{{\mathbb S}_\infty^{d-1}}
  \left(\bigvee_{j} \theta_jz_j\right) d\Omega(\theta)\; ,\]
  which is the reason $\Omega$ is a spectral measure.}
  Then we can think of the spectral measure in terms of the limit measure $\mu$, and:
  \begin{equation}
    \mu[{\bf z}:R(z)>t, {\bf V}\in B] = t^{-1}\Omega(B).
  \end{equation}
  Thus we see a one-to-one correspondance between the limit measure $\mu$ and the
    spectral measure $\phi$, and by factoring out the Pareto distributed radial
    component, we can establish a distribution on the angular component
  \begin{equation}
    \text{Pr}\left({\bf V} \in B \mid r > 1\right) = \frac{\Omega(B)}{\Omega(S_{\infty}^{d-1})},
  \end{equation}
  \bruno{Again, this needs sharpening, I don't get the $r>1$. What you have is that, for any value of $r$.
  \begin{equation}
   \lim_{r\rightarrow\infty} \text{Pr}\left({\bf V} \in B \mid R > r\right) =
              \frac{\Omega(B)}{\Omega(S_{\infty}^{d-1})},
  \end{equation}
  So, the point here is what we do in practice. We start by doing the univariate analysis for each
  component. Take $b_{t,j} = F^{-1}(1 - 1/t)$ as the threshold. So
  \begin{equation}
   \lim_{t\rightarrow\infty} \text{Pr}\left({\bf V} \in B \mid R_t > 1\right) =
            \frac{\Omega(B)}{\Omega(S_{\infty}^{d-1})},
  \end{equation}.
  What this means in practice is that we take high thresholds in each dimension, and then take the standardized vectors for which $R>1$.}
  \makenote{Paraphrasing from Goix et. al., figure if/how to cite}
  conditioned on at least one of the components being extreme in the marginal
  sense.  It is using this property that we will establish our method.

For each $\nu\subset\lbrace{1,\ldots,d}, \nu \neq \emptyset$, we define the
    \emph{truncated cone} $\mathcal{C}_{\nu}$, where
  \begin{equation}
      \mathcal{C}_{\nu} = \left\lbrace {\bf z} \geq 0 : \lVert z\rVert_{\infty}\geq 1, z_j \geq 0 \forall j \in \nu, z_j = 0 \forall j \not\in \nu\right\rbrace.
  \end{equation}
  That is, $\nu$ identifies a set index specifying components of the standardized
  data for which the observation is greater than 0.  The observation is greater
  than 0 for columns within that index, and 0 outside that index.  By
  construction, we're also requiring that the observation is greater than 1 in
  at least one of those dimensions.  By this definition, we observe that each
  $\mathcal{C}_{\nu}$ is distinct and disjoint from any other
  $\mathcal{C}_{\nu}$.  Now, defining $\Omega_{\nu}$ as the projection of
  $C_{\nu}$ onto $S_{\infty}^{d-1}$,
  \begin{equation}
      \Omega_{\nu} = \left\lbrace {\bf v} \in S_{\infty}^{d-1} : x_i > 0 \forall i \in \nu, x_i = 0 \forall i \not\in \nu\right\rbrace,
  \end{equation}
  we can clearly see $\mu(\mathcal{C}_\nu) = \Phi(\Omega_{\nu})$ for all
  $\alpha \subset\lbrace1,\ldots,d\rbrace$.  Where we call $\mu(\dot)$ the limit
  measure, we refer to $\Phi(\dot)$ as the \emph{spectral} or
  \emph{angular measure}.  Our goal is establishing statistical inference on this
  angular measure.




% EOF
