\subsection{Model Comparison on the Hypercube}
It is not immediately obvious which criteria to use to judge these models and decide which best
  represents the data's generating distribution.  We have opted to use the
  \emph{posterior predictive loss} criterion of \cite{gelfand1998} and the
  \emph{energy score} criterion of \cite{gneiting2007}.  Both of these
  metrics require calculating some distance in the target space, and this section will be devoted
  to that end.

\subsubsection{Posterior Predictive Loss Criterion}
The posterior predictive loss criterion, \emph{ppl} is introduced in \cite{gelfand1998}.  When we assume a
  squared error loss function, then for the $l$th observation, the posterior predictive loss criterion
  is computed as
  \begin{equation}
    \label{eq:ppl}
    D_k = \text{Var}(X_l) + \frac{k}{k + 1}\left(\text{E}[X_l] - {\bf x}_l\right)^2,
  \end{equation}
  where $X_l$ is a random variable from the posterior predictive distribution for $x_l$.  The
  scalar $k$ is a weighting factor by which we arbitrarily scale the importance of goodness of fit
  relative to precision.  In our analysis, we take the limit as $k\to\infty$, and thus weight both
  parts equally.  Interpreting this criterion, a smaller $\text{Var}({\bf X}_l)$ indicates a higher
  precision, and a smaller $(\text{E}[{\bf X}_l] - {\bf x}_l)^2$  indicates a better fit.  Thus,
  smaller is better.  Note that this is defined for a univariate distribution--we are going to
  generalize this somewhat, to account both for the multivariate nature of our distribution, and
  its constrained geometry.  That is, let us re-define it as
  \begin{equation}
    \label{eq:ppl2}
    D_k^{\prime} = \text{E}\left\lVert{\bf X_l},\text{E}[{\bf X}_l]\right\rVert_{\Omega}^2 +
                    \frac{k}{k+1}\left\lVert\text{E}[{\bf X}_l],{\bf x}_l\right\rVert-{\Omega}^2.
  \end{equation}
  This generalizes the posterior predictive variance as expected squared distance from a central
  mean.  We can numerically calculate this mean as \makenote{Not sure of this.  googling yields
   this as the result for median...at least in 1 dimension.  not sure how to calculate for mean.}
  \begin{equation*}
    \text{E}[{\bf X}_l] = \text{argmin}_{\omega \in \Omega}\text{E}\lVert {\bf X}_l, s \rVert_{\Omega}.
  \end{equation*}
  This is numerically difficult. Adjusting our interpretation somewhat, we can also project our
  replicates of ${\bf X}_l$ onto the simplex, calculate the mean on the simplex, then re-project
  that point onto the hypercube.

\subsubsection{Energy Score}
The energy score of \cite{gneiting2007} is a generalization of the continuous ranked probability
  score, or \emph{crps}, defined for a multi-dimensional random variable.
  \begin{equation}
    \label{eq:es}
    \text{ES}(P,x) = \frac{1}{2}\text{E}_p\lVert {\bf X}_l,{\bf X}_l^{\prime}\rVert_{\Omega}^{\beta} -
                            \text{E}_p\lVert {\bf X}_l - {\bf x}_l\rVert_{\Omega}^{\beta}
  \end{equation}
  where ${\bf X}_l^{\prime}$ is another replicate from the posterior predictive distribution
  of ${\bf x}_l$. This means, rather than relying on the first and second moments of the posterior
  predictive distribution as in the case of posterior predictive loss, we are instead calculating
  pairwise distances between the observation and draws from the posterior predictive distribution,
  as well as pairwise distances between those replicates themselves.

Now here's the rub.  We are not aware of any standardized distance metrics developed on the
  positive orthant of the unit hypercube.  In the unit simplex, we can assume the use of
  Euclidean norm.  on the unit hypersphere, our task would be slightly more difficult as
  Euclidean norm would under-report the actual distance required for travel between points $a_1$
  and $a_2$.  On the unit hypercube, where our task is defined, the distortion between Euclidean
  norm and the actual distance travelled will be even greater.

The positive orthant of the unit hypercube, defined in Euclidean geometry, is that structure for
  which, in a given point on the hypercube, all dimensions of that point are between 0 and 1, and
  at least one dimension must be 1.  Developing terminology, we can consider observations for which
  the $j$th dimension is equal to 1, to be on the $j$th \emph{face}.  The intersection of the $i$th
  and $j$th face is a hypercube with $d-2$ degrees of freedom, and observations in this space have
  dimensions $i,j$ equal to 1.

A \emph{distance} in this space corresponds to a \emph{geodesic} on this space. From geometry, we
  know that the geodesic, or shortest path between 2 points along the surface of a $d$ dimensional
  figure corresponds to at least one \emph{unfolding}, or \emph{rotation} of the $d$-dimensional
  figure into a $d-1$ dimensional space.  The appropriate term for the structure generated by this
  unfolding is a \emph{net}.  For the appropriate net, a line segment connecting the two points and
  staying within the boundaries of the net corresponds to the shortest path between those points
  \makenote{needs citation!}, and is thus a geodesic.  The length of that line segment is properly
  defines the distance required for travel between those points.

Consider a 3-dimensional cube.  Consider 2 points on this 3-dimensional cube,
  ${\bf a}_1 = (x_1,y_1,z_1)$, and ${\bf a}_2 = (x_2,y_2,z_2)$. Let's say that the two points are
  on the same face.  Then the distance between those two points, the distance one has to travel
  along the space to move from one point to the other, is calculated by Euclidean norm.  Now,
  consider two points on separate faces.  All faces are pairwise adjacent, as we have stated, so
  in order to move to the other point, we must \emph{at least} move to the intersection between the
  faces, then to the other point.  Let ${\bf a}_1$ lie on the $x$ face, and ${\bf a}_2$ lie on the
  $y$ face.  That is, ${\bf a}_1 = (1, y_1, z_1)$, ${\bf a}_2 = (x_2, 1, z_2)$.  Then traveling
  between these points we must at least pass through the intersection of faces $x,y$.  One possible
  net representation of this is unfolding the $y$ face alongside the $x$ face.  We accomplish this
  by applying a rotation and translation to $a_2$, corresponding to the following:
  \begin{equation}
    \label{eq:1drotation}
    a_2^{\prime} = \begin{bmatrix}
    1 \\
    2 \\
    0
    \end{bmatrix}
    +
    \begin{bmatrix}
    0  & 0 & 0 \\
    -1 & 0 & 0 \\
    0  & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    x_2 \\
    1 \\
    z_2 \\
    \end{bmatrix} = \begin{bmatrix}
    1 \\
    2 - x_2 \\
    z_2
    \end{bmatrix}
  \end{equation}
  Then, if this is the appropriate net, the distance becomes
  $\lVert {\bf a}_1,{\bf a}_2\rVert = \lVert {\bf a}_1 - {\bf a}_2^{\prime}\rVert_2$ However, there
  is another possible net we must consider, travelling first through the $z$ face then to the $y$
  face.  The rotation for that becomes:
  \begin{equation}
    \label{eq:2drotation}
    a_2^{\prime} = \begin{bmatrix}
    1 \\
    2 \\
    2
    \end{bmatrix}
    +
    \begin{bmatrix}
    0 & 0 & 0  \\
    0 & 0 & -1 \\
    -1 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
    x_2 \\
    1 \\
    z_2
    \end{bmatrix} = \begin{bmatrix}
    1 \\
    2 - z_2 \\
    2 - x_2
    \end{bmatrix}
  \end{equation}
Every successive rotation is relative to the last face.  So, as the number of dimensions grows, the
  nmumber of possible rotations grows as well.

  \makenote{need to finish this!}

As we have $d$ dimensions, if 2 observations are on different faces, then there are
  $\sum_{j = 1}^{d-2}\binom(d-2,j) + 1$ possible rotations to consider. \makenote{There are truly $d!$
  possible nets, but when we consider starting and ending faces fixed, and that portions of the net
  that diverge after the ending face are irrelevant, we arrive at that number of rotations that we
  actually need consider}.  This is not insurmountable, it is numerically difficult, and developing
  the generalized rotation strategies for $d$ dimensions is beyond the scope of this analysis.
  However, we are in luck in that all we need for a valid energy score is a
  \emph{negative definite kernel}.  This is defined as a function having symmetry in its arguments,
  $d(x_1,x_2) = d(x_2,x_1)$, and for which $\sum_{i =1}^n\sum_{j=1}^na_ia_jd(x_1,x_2) \leq 0$ for
  all positive integers n, with the restriction that $\sum_{i=1}^na_i = 0$.  The Euclidean norm
  is one example of a negative definite kernel.

Let's go back to the first rotation--we held that as a numerically easier analogue to what we were
  actually calculating--the distance from the starting point, to some optimal point along the
  intersection between the $x$ and $y$ planes, to the ending point.  That is, the sum of two
  Euclidean norms.



% EOF

