\section{Model Comparison on the Hypersphere}
It is not immediately obvious what criteria to use to judge these models and decide which best
  represents the data's generating distribution.  As the task resides on $\mathcal{S}_{\infty}^{d-1}$,
  our options are somewhat limited in terms of what model criteria we can use.  Because of the aforementioned
  difficulty in evaluating a density directly in this space, we choose to use model evaluation
  criteria that can operate on samples from the posterior predictive distribution.  For this reason,
  we have opted to use the \emph{posterior predictive loss} criterion of \cite{gelfand1998} and the
  \emph{energy score} criterion of \cite{gneiting2007}.  Both of these metrics require calculating
  some distance in the target space, and this section will be devoted to that end.  As both these
  metrics operate on the posterior predictive distribution for each observation, we will also be
  including a metric operating on the overall posterior predictive distribution, in the form of a
  modified Kullbeck-Liebler divergence.

\subsection{Posterior Predictive Loss Criterion}
The posterior predictive loss criterion, \emph{PPL} is introduced in \cite{gelfand1998}.  When we
  assume a squared error loss function, then for the $i$th observation, the posterior predictive
  loss criterion is computed as
  \begin{equation}
    \label{eq:ppl}
    D_{k}^{(i)} = \text{Var}(X_i) + \frac{k}{k + 1}\left(\text{E}[X_i] - x_i\right)^2,
  \end{equation}
  where $X_i$ is a random variable from the posterior predictive distribution for $x_i$.  The
  scalar $k$ is a weighting factor by which we arbitrarily scale the importance of goodness of fit
  relative to precision.  In our analysis, we take the limit as $k\to\infty$, and thus weight both
  parts equally.  Interpreting this criterion, a smaller $\text{Var}({\bf X}_l)$ indicates a higher
  precision, and a smaller $(\text{E}[{\bf X}_l] - {\bf x}_l)^2$  indicates a better fit.  Thus,
  smaller is better.  Note that this is defined for a univariate distribution.  As we are dealing
  with a multivariate distribution, in keeping with our spatial statistics brethren
  \makenote{find a citation where they do this}, we will be taking the posterior predictive loss
  summed over all dimensions.  That is,
  \begin{equation}
    \label{eq:ppl2}
    D_k^{(i)} = \frac{1}{d}\sum_{l = 1}^{d}\left[\text{Var}(X_{il}) + \frac{k}{k+1}\left(\text{E}[X_{il}] - x_{il}\right)^2\right]
  \end{equation}
  Then we report the average $D_k$, taken over all $i$.

\subsection{Energy Score}
The energy score of Gneiting, et al\cite{gneiting2007}, is a generalization of the continuous ranked
  probability score, or \emph{crps}, defined for a multi-dimensional random variable.
  \begin{equation}
    \label{eq:es}
    \text{ES}(P,x_i) = \frac{1}{2}\text{E}_p g(\bm{X}_i,\bm{X}_i^{\prime}) - \text{E}_p g(\bm{X}_i, \bm{x}_i)
  \end{equation}
  where $g(\cdot)$ identifies a negative definite kernel, and $\bm{X}_i^{\prime}$ represents another
  replicate from the posterior predictive distribution of $\bm{x}_i$. The negative definite kernel
  most often used is Euclidean distance.  However,on $\mathcal{S}_p^{d-1}$ for $p > 1$, Euclidean
  distance can under-report the actual distance required for travel between two points. That distortion
  is maximized on $\mathcal{S}_{\infty}^{d-1}$.

\subsection{Distance on the positive orthant of the Hypercube}
  \label{subsec:distance}
The positive orthant of the unit hypercube, defined in Euclidean geometry, is that structure for
  which, in a given point on the hypercube, all dimensions of that point are between 0 and 1, and
  at least one dimension must be 1.  Developing terminology, we can consider observations for which
  the $j$th dimension is equal to 1, to be on the $j$th \emph{face}.  The intersection of the $i$th
  and $j$th face is a $d-2$ dimensional cube, and observations in this space have dimensions $i,j$
  equal to 1.

A \emph{distance} in this space is a \emph{geodesic} on this space. From geometry, we know that
  the geodesic, or shortest path between 2 points along the surface of a $d$ dimensional figure
  corresponds to at least one \emph{unfolding}, or \emph{rotation} of the $d$-dimensional
  figure into a $d-1$ dimensional space.  The appropriate term for the structure generated by this
  unfolding is a \emph{net}.  For the appropriate net, a line segment connecting the two points and
  staying within the boundaries of the net corresponds to the shortest path between those points
  \makenote{needs citation!}, and is thus a geodesic.  The length of that line segment is properly
  defines the distance required for travel between those points.

Consider a 3-dimensional cube.  Consider 2 points on this 3-dimensional cube,
  ${\bf a}_1 = (x_1,y_1,z_1)$, and ${\bf a}_2 = (x_2,y_2,z_2)$. Let's say that the two points are
  on the same face.  Then the distance between those two points, the distance one has to travel
  along the space to move from one point to the other, is calculated by Euclidean norm.  Now,
  consider two points on separate faces.  All faces are pairwise adjacent, as we have stated, so
  in order to move to the other point, we must \emph{at least} move to the intersection between the
  faces, then to the other point.  Let ${\bf a}_1$ lie on the $x$ face, and ${\bf a}_2$ lie on the
  $y$ face.  That is, ${\bf a}_1 = (1, y_1, z_1)$, ${\bf a}_2 = (x_2, 1, z_2)$.  Then traveling
  between these points we must at least pass through the intersection of faces $x,y$.  One possible
  net representation of this is unfolding the $y$ face alongside the $x$ face.  We accomplish this
  by applying a rotation and translation to $a_2$, corresponding to the following:
  \begin{equation}
    \label{eq:1drotation}
    a_2^{\prime} = \begin{bmatrix}
    1 \\
    2 \\
    0
    \end{bmatrix}
    +
    \begin{bmatrix}
    0  & 0 & 0 \\
    -1 & 0 & 0 \\
    0  & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    x_2 \\
    1 \\
    z_2 \\
    \end{bmatrix} = \begin{bmatrix}
    1 \\
    2 - x_2 \\
    z_2
    \end{bmatrix}
  \end{equation}
  Then, if this is the appropriate net, the distance becomes
  $\lVert {\bf a}_1,{\bf a}_2\rVert = \lVert {\bf a}_1 - {\bf a}_2^{\prime}\rVert_2$ However, there
  is another possible net we must consider, travelling first through the $z$ face then to the $y$
  face.  The rotation for that becomes:
  \begin{equation}
    \label{eq:2drotation}
    a_2^{\prime} = \begin{bmatrix}
    1 \\
    2 \\
    2
    \end{bmatrix}
    +
    \begin{bmatrix}
    0 & 0 & 0  \\
    0 & 0 & -1 \\
    -1 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
    x_2 \\
    1 \\
    z_2
    \end{bmatrix} = \begin{bmatrix}
    1 \\
    2 - z_2 \\
    2 - x_2
    \end{bmatrix}
  \end{equation}
Every successive rotation is relative to the last face.  So, as the number of dimensions grows, the
  number of possible rotations grows as well.  As we have $d$ faces, if 2 observations are on
  different faces, then there are $\sum_{j = 1}^{d-2}\binom{d-2}{j} + 1$ possible rotations to
  consider. \footnote{There are truly $d!$ possible nets, but when we consider starting and ending
  faces fixed, and that portions of the net that diverge after the ending face are irrelevant,
  we arrive at that number of rotations that we need consider}.  While this is not
  insurmountable, it is numerically difficult, and developing the generalized rotation strategies
  for $d$ dimensions is beyond the scope of this analysis. However, we are in luck in that all we
  need for a valid energy score is a \emph{negative definite kernel}.  This is defined as a function
  having symmetry in its arguments, $d(x_1,x_2) = d(x_2,x_1)$, and for which
  $\sum_{i =1}^n\sum_{j=1}^na_ia_jd(x_1,x_2) \leq 0$ for all positive integers n, with the
  restriction that $\sum_{i=1}^na_i = 0$.  The Euclidean norm is one example of a negative definite
  kernel.

Let's go back to that first rotation--we held that as a numerically easier analogue to our actual
  goal--the distance from the starting point, to some optimal point along the intersection between
  the starting and ending faces, to the ending point.  At that optimal point, the total distance
  travelled between starting and ending points becomes symmetric.  We can
  recognize this distance as the sum of two Euclidean norms.  That is,
  \begin{equation}
    \lVert {\bf a},{\bf b}\rVert_{H} = \lVert {\bf a}, {\bf c} \rVert_2 + \lVert {\bf c},{\bf b}\rVert_2
  \end{equation}
  If we can be assured of symmetry in the functional arguments, then the requirements for a negative
  definite kernel are trivially proved.  And from geometry\makenote{need citation on this}, we can
  assert that on a one-dimension unfolding, the line segment connecting the starting and ending
  points will travel through that optimal net.

  \bruno{This whole paragraph can be made much shorter and crispier. Introduce the problem, then mention the
         difficulty of defining an actual distance in $S_\infty^{d-1}$ and jump into the negative kernel idea,
         then write the lemma from the paper by Gneiting and Raftery, define a suitable kernel and write a
         proposition proving that it is negative definite, then comment on how you calculate it.
         BTW, I still don't understand the notation ||a,b||.}

% \subsubsection{Intrinsic Energy Score}
% While we have at length discussed our means of comparison between models, we have limited means of
%   comparing how our model is doing relative to the data.  We introduce the
%   \emph{intrinsic energy score} to offer a baseline energy score \emph{in the data}, against which
%   we might compare candidate models.  This allows us to see how well our model is doing relative
%   to the data, rather than just relative to other models.  We construct this using the energy score
%   metric, but for any particular observation in the data, we compare that observation against all
%   other observations in the data.  That is, for a given observation, treat other observations as
%   replicates of that observation.  Comparing our energy score results against this value offers a
%   metric of how well we are partitioning the data into separate distributions to be evaluated.

\subsection{Kullbeck Liebler Divergence}
The \emph{Kullbeck Liebler divergence} $D_{\text{KL}}$ offers a means of comparing two densities.
  It is, at its core, a log-ratio of the two densities, integrated over the first density.  That is,
  \begin{equation}
    \label{eqn:kld}
    D_{\text{KL}}(A,B) = \int_{x\in \Omega(A)}A(x)\log\left(\frac{A(x)}{B(x)}\right)dx
  \end{equation}
  where $\Omega(\cdot)$ indicates the support, and $A,B$ are densities.  As the KL divergence is not
  symmetric, it is not referred to as a \emph{distance} between densities, but it is often employed
  as a means of comparison.  In our case, as the densities are intractible, we will be computing an
  estimate of the KL divergence using a numerical method of estimating the density.

Parzen \cite{parzen1962} offers a method of density estimation involving setting a window with
  radius $\sigma$ around a point, and counting observations within that window.  The number of
  observations within the window, divided by the total number of observations within the dataset,
  and the area of the window provides an estimate of the density at that point. This is highly
  dependent upon the choice of window radius $\sigma$.  Furthermore, the Parzen method suffers
  another issue--the curse of dimensionality.  \cite{boltz2009} shows, for finite sample size, that
  as the number of dimensions increases, the probability of observations falling into the window on
  the other dataset decreases. Effectively, the quality of density estimation goes down as
  dimensionality goes up.

An alternative to this method that we will use is based on distance between observations from the
  empirical and posterior predictive distributions.  Similar to the Parzen method, we're working
  with samples from the distribution, but instead of establishing a window of some width, we are
  using distance from the target observation to its $k$th nearest neighbor in the dataset.
  From hence we get the name \emph{KNN Based KL Divergence}.  As we operate on the hypersphere,
  we again recognize that the Euclidean distance metric is not appropriate, delivering results strongly
  biased downwards.  Our one-rotation kernel method described previously is employed here as a
  distance.  We recognize that the one-rotation norm is on average biased slightly longer as
  compared to a proper geodesic, but it is closer to truth than any other distance metric we are
  aware of.  To that end, we employ it as the pairwise distance measure required by KNN for this KNN
  based KL divergence.  We employ the formula of Boltz et al \cite{boltz2009}, as
  \begin{equation}
    \label{eqn:knnkld}
    D_{\text{KL}}^{(k)}(A,B) = \log\frac{n(B)}{n(A)} + c(A) \left[\rho_A^{(k)}(B)
                                                                      - \rho_A^{(k)}(A)\right]
  \end{equation}
  where $\rho_A(\cdot)$ denotes the average log-distance for observations in $A$ to their $k$th
  nearest observation in $\cdot$.  Boltz et al acknowledge that their method is biased, does not
  integrate to 1, but its disadvantages in bias as compared to the Parzen windowing method are less
  apparent at higher dimensions, where the windowing method's weakness to dimensionality tends to
  rise.  As we regard this as a numerical approximation, we offer this metric in the spirit of
  complementary evidence, rather than an authority, or oracle, to show us the way.































% EOF
