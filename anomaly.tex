\section{Applications to Anomaly Detection}
\subsection{Background}
Anomaly detection describes a field of methods for identifying observations as \emph{anomalous}.  That
  is to say, observations which do not fit the general distribution of the data from which they came.
  Equivalent names for this field are \emph{outlier detection}, and \emph{novelty detection}, though
  some authors in this field will separate the use of those terms by whether or not it is assumed
  that anomalous observations are included in the training dataset.  Crucial to this distinction is
  the existence of a labelled training data set, so that one could effect supervised learning.

For our purpose, we do not assume the existence of labels in the training dataset, and seek an
  algorithm that can produce anomaly scores in the absence of class labels. As such, we will offer
  a brief overview of unsupervised anomaly detection methods, as well as discussion of the methods
  we are preparing here as competing models.

\subsubsection{Statistical Models}
Common to most methods of anomaly detection is the belief that data in regions of relative data sparsity
  are more likely to be anomalous.   To find these regions of sparsity--or more accurately, to find
  which observations are in these regions of sparsity--we generate models to describe the distributional
  density around each observation.  Observations in region of low estimated density are more likely to
  be anomalous.  This subfield includes all manner of non-parametric kernel density estimation such
  as binned histograms, Parzen window methods, along with parametric methods.

\subsubsection{Clustering Methods}
Clustering methods tend to sort data into groups \emph{near} each-other.  These rely on \emph{distance}
  metrics and tend to make no assumptions regarding the underlying distribution of the data.  We can
  further sub-divide this sub-field into density-based, centroid-based, and linkage-based clustering
  methods.

Linkage-based clustering methods group data based on pairwise distance point-to-point, or between
  elements of clusters.  An illustrative example is single linkage, where the distance between two
  clusters is defined as the minimum distance between a point in each set.   Similarly, complete
  linkage defines the metric to be the maximum pairwise between a point in each set. \findcite

Centroid based clustering methods such as $k$-Means instead generate $k$ cluster centroids according
  to some metric, then an observation's \emph{anomaly} score is a function of the distance from that
  observation to the nearest cluster centroid.  More advanced clustering algoritms will look at
  distance to nearest edge, or some combination thereof.  The algorithm used to find the cluster
  centroids is also variable, changing what metric is optimized in finding the location of the
  centroids.  In this subfield, we find the aforementioned $k$-means.

Density based methods will use pairwise distances to establish some measure of local density, then
  establish local modes as clusters.  \emph{DBSCAN} \citep{ester1996} follows this, by forming
  neighborhoods of observations and establishing labels based on the neighborhood.

\subsubsection{Non-Statistical Models}
There are additional non-statistical families of methods beyond clustering, adapted from the realm
  of general classification models to unsupervised learning.  The Isolation Forest,\citep{liu2000}
  adapted from random forests~\citep{breiman2001} uses decision trees to isolate observations.  Those
  observations that are more easily isolable are regarded as more anomalous.  One-class SVM is a
  variant of the support vector machine classification system, where instead of optimizing a
  decision boundary between labelled data, it wraps observed data in a decision boundary.  The
  anomaly score details how close an observation comes to the decision boundary.
