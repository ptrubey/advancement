\section{Applications to Anomaly Detection}
\label{sec:anomaly}
Anomaly detection describes a field of methods for identifying observations as \emph{anomalous}.  That
  is to say, observations which do not fit the general distribution of the data from which they came.
  Equivalent names for this field are \emph{outlier detection}, and \emph{novelty detection}, though
  some authors will separate the use of those terms by whether or not it is assumed that anomalous
  observations are included in the training dataset.  Crucial to this distinction is the assumption
  of a \emph{clean} training data set.

The applicability of extreme value theory to anomaly detection is predicated on the belief that
  extreme observations are more susecptible to anomalies.  \cite{goix2017} provides a discussion to
  this effect, that extreme observations exist at the border between anomalous and non-anomalous regions.

For our purpose, we do not assume the existence of labels in the training dataset, and seek an
  algorithm that can produce anomaly scores in the absence of class labels. As such, we will offer
  a brief overview of unsupervised anomaly detection methods, as well as discussion of the methods
  we are preparing here as competing models.

\subsection{Existing Methods}
The complete field of anomaly detection is wide, and we could not hope to provide a comprehensive review
  it.  However, most methods can be roughly grouped into three main ideas: statistical models, clustering
  methods, and non-statistical models. Core to the development of a method lies a basic question,
  \emph{what is an anomaly?}  The answer to this question depends on application and what we hope to
  learn.  For instance, in a regression setting, observations that produce large outliers may be considered
  anomalous.  In a classification setting, common to most methods of anomaly detection is the belief
  that data belonging to regions of relative data sparsity are more likely to be anomalous.  Methods in
  this space work to find these regions of relative data sparsity, or equivalently, regions of relative
  data abundance.

\subsubsection{Statistical Models}
Statistical modelling for anomaly detection employs statistical models to describe the distribution of
  data.  The goal from the model can be to estimate the data density around an observation (knn, )\findcite.
  The assumption made is that observations in regions of low estimated density are more likely to be
  anomalous.  Statistical modelling as a field includes parametric density estimation routines This
  subfield includes all manner of non-parametric density estimation such as binned histograms, kernel
  methods such as the Parzen window method, along with parametric methods such as Gaussian mixture models.

\subsubsection{Clustering Methods}
Clustering methods tend to sort data into groups \emph{near} each-other.  These rely on \emph{distance}
  metrics and tend to make no assumptions regarding the underlying distribution of the data.  We can
  further sub-divide this sub-field into density-based, centroid-based, and linkage-based clustering
  methods.

Linkage-based clustering methods group data based on pairwise distance point-to-point, or between
  elements of clusters.  An illustrative example is single linkage, where the distance between two
  clusters is defined as the minimum distance between a point in each set.   Similarly, complete
  linkage defines the metric to be the maximum pairwise between a point in each set. \findcite

Centroid based clustering methods such as $k$-Means instead generate $k$ cluster centroids according
  to some metric, then an observation's \emph{anomaly} score is a function of the distance from that
  observation to the nearest cluster centroid.  More advanced clustering algoritms will look at
  distance to nearest edge, or some combination thereof.  The algorithm used to find the cluster
  centroids is also variable, changing what metric is optimized in finding the location of the
  centroids.  In this subfield, we find the aforementioned $k$-means.

Density based methods will use pairwise distances to establish some measure of local density, then
  establish local modes as clusters.  \emph{DBSCAN} \citep{ester1996} follows this, by forming
  neighborhoods of observations and establishing labels based on the neighborhood.

\subsubsection{Non-Statistical Models}
There are additional non-statistical families of methods beyond clustering, adapted from the realm
  of general classification models to unsupervised learning.  The Isolation Forest,\citep{liu2000}
  adapted from random forests~\citep{breiman2001} uses decision trees to isolate observations--the
  observations that are more easily isolable are regarded as more anomalous.  One-class SVM is a
  variant of the support vector machine classification system optimized towards anomaly detection.
  In a similar manner to the support vector machine using support vectors to describe a decision
  boundary separating classes of data, the one-class SVM uses support vectors to describe a decision
  boundary around \emph{normal} behavior.  A higher distance to that decision boundary on the
  anomalous side is regarded as \emph{more anomalous}.

\makenote{Add more citations!}

\subsection{Proposed Scoring Methods}
We offer two adaptations of scoring methods for use on the hypersphere.  As we have previously
  mentioned, an evaluable density is difficult to create on $\mathcal{S}_{\infty}^{d-1}$.

\subsubsection{Density Estimation on the hypersphere}
One way we can think of anomalies is as observations that are unlikely to occur.  Given a fitted model,
  those observations that come from a low density area may be more likely to be \emph{anomalous}.
  As we do not have a density available in closed form, we turn to a non-parametric kernel density
  estimation method for establishing local density, based on KNN.  We generate the inverse scores as
  local posterior predictive density (as established by the KNN estimator), times the probability of
  observing

We establish the local posterior predictive density under the KNN estimator by assuming a locally
  uniform density within a $d-1$ dimensional ball, centered on $V_i$, with a radius defined by
  the hypercube distance estimation metric detailed in~\ref{subsec:distance}.
  \begin{equation}
    \label{eqn:ad_knn}
      S_{i,(k)}^{-1} =
        \frac{k}{N}\frac{\Gamma\left(\frac{d-1}{2} - 1\right)}{\pi^{\frac{d-1}{2}}D_{k}^{d-1}(V_i)}
  \end{equation}
  This provides an estimator of local density that we can use to establish a scoring metric.

\subsubsection{Contribution to posterior predictive loss}
Another way we can think of anomalies is as observations that are difficult to fit--that is, difficult
  for a fitted model to predict.  We assume that the greater the difficulty encountered in fitting a
  given observation, the more anomalous it is.  The posterior predictive loss criterion in
  Equation~\ref{eq:ppl} provides one such metric.  The first term measures the posterior predictive
  variance--a larger variance corresponds to less precision in the prediction, or alternatively, a lower
  density in the predictive distribution.  The second term measures squared bias--the squared distance
  between the average predicted value, and the observation.  Both of these terms provide a metric of
  how \emph{difficult} a particular observation is to fit.

\subsection{Anomaly Detection Performance on the hypersphere}
\makenote{This needs to be presented as solely preliminary analysis}
In a relative dearth of real-valued \emph{labelled} sample anomaly detection datasets, we have chosen
  to simulate AD datasets for comparison of our AD algorithms.  Generation of these datasets
  follows Algorithm~\ref{algo:simulated} for 95 percent of the data, then mutates the generating
  distribution by rotating axis labels by 1 for each mixture component--so $\beta{j,l}$ becomes
  $\beta_{j,l+1}$.  The remaining 5 percent of the data is simulated from this mutation.
  We have 5 and 10 column datasets, each with 5 and 10 mixture components in the \emph{normal} data.

\begin{table}[h]
  \centering
  \label{tab:ad_sim_results}
  \include{table_ad_sim_results}
  \caption{Area under the ROC Curve for various methods, established on simulated data.  }
\end{table}

The higher the number of mixture components, the more comparatively \emph{noisy} the data.  In such
  an environment, anomalies are more difficult to spot.  We see a relative performance drop moving
  from 5 to 10 mixture components, across all scoring algorithms.





% EOF
