% results.tex
\subsection{Integrated Vapor Transport}
We use data from the integrated water vapor transport (\emph{IVT}) model\makenote{needs citation}
  of atmospheric rivers as a means of testing these models.  An atmospheric river is a meteorological
  event of local concentration of water vapor in the atmosphere that moves with wind patterns.
  Understanding extremal dependence in atmospheric rivers, for instance, grants hydrological engineers
  a greater understanding of how and where to build ground water capacity to handle precipitation.
  California suffers persistent, extended droughts--if ground water transportation and holding capacity
  (canals, reservoirs) are not built to manage extreme precipitation without adequate understanding of
  the extremal dependence, then we may not install adequate capacity and thus experience flooding.
  Alternatively, if we install too much capacity everywhere, then we are wasting money that could be
  better spent elsewhere.

Fitting our models to this data requires some pre-processing.  The marginal distributions of the
  IVT data appear naturally log-normal, which falls into the domain of attraction of a Gumbel
  distribution.  Given that, we can apply a high threshold, and exceedances over that threshold can
  be modelled as Generalized Pareto.  Our modelling begins with that assessment.  As estimating
  the Pareto parameters is not yet our focus in this analysis, we choose to apply the threshold
  using the empirical CDF.  That is, for a given $t$, let $b_{tl} = \hat{F}_l^{-1}(1 - t^{-1})$.  For
  this analysis, we set $t = 20$, indicating the marginal $95$ percentile.  The other parameters of the
  generalized Pareto--the scale parameter $\alpha_{tl}$ and the extremal index $\chi_l$--are set via
  maximum likelihood.  A fully Bayesian model formulation will allow their varying within a
  distribution, but such a model formulation will not be conducive to fitting by Markov-chain Monte
  Carlo methods.

After the thresholding and maximum likelihood estimation of the parameters of the Pareto, we scale
  the data to the standard multivariate Pareto.  Dividing each standardized observation by its
  $\mathcal{L}_{\infty}$ norm, we project the standardized data onto $\mathcal{S}_{\infty}^{d-1}$.
  Data in sequence represents observations in time, and these are heavily correlated.  As such, we
  choose to \emph{decluster} the observations, such that, by observing a sequence of observations
  $\bm{ z}$ for which $\inorm{z_i} > 1$ for each observation in sequence, we keep only the observation
  with the greatest observed $\mathcal{L}_{\infty}$ norm.  The complete procedure is outlined in
  Algorithm~\ref{algo:processing}.

\begin{algorithm}
  \label{algo:processing}
  \KwResult{$\bm{ r} : r_i \sim \text{Pareto}(1)$, $\bm{ v} \in \mathcal{S}_{\infty}^{d-1}$}
  \For{$l = 1,\ldots,d$}{
    Set $b_{tl} = \hat{F}_l^{-1}\left(1 - \frac{1}{t}\right)$.\\
    With $\bm{ x}_l > b_{tl}$, fit $a_{tl}$, $\chi_l$ via MLE according to generalized Pareto likelihood.\\
    }
  \For{$i = 1,\ldots,n$}{
    Define $z_{il} = \left(1 + \xi_l\frac{x_{il} - b_{t,l}}{a_{t,l}}\right)_{+}^{1/\xi_l}$\\
    Define $r_i = \pnorm{\bm{ z}_i}{\infty}$, $\bm{ V}_i = \frac{\bm{ z}_i}{\pnorm{\bm{ z}_i}{\infty}}$\\
    }
  Subset $\bm{ r},\bm{ v}$ such that $r_i \geq 1$ for all $r_i\in \bm{r}$.\\
  \If{declustering}{
    \For{$i = 1,\ldots,n$}{
      If $r_i \geq 1$ and $r_{i-1} \geq 1$, drop the lesser (and associated $v_i$) from dataset.\\
    }
  }
 \caption{Data pre-processing to isolate and transform data exhibiting extreme behavior.  $r_i$ represents the radial component, 
    and $\bm{v}_i$ the angular component.  The declustering portion is relevant if data is correlated in time.}
\end{algorithm}

We have, at our disposal, two datasets from the IVT model. \bruno{Did you use daily data? For how many years? You should say how many data you started with, and how many you ended up with after you did the clustering. Also, were you not looking only at the rainy season for each year?}  One records data from 8 grid cells, covering
  generally the coast of California.  The other, with a higher resolution, records data from 47 grid
  cells covering the same area.  We fit our models to both datasets.

\begin{table}[hb]
  \label{tab:dev}
  \centering
  \include{table_dev}
  \caption{Model comparison metrics: Posterior Predictive Loss and Energy Score criteria from fitted
    models against the IVT data.  All presented models are DP mixtures; the \emph{Model} field
    identifies the kernel distribution.  For both criteria, lower is better.}
\end{table}

In Table~\ref{tab:dev} we see the same model preference towards the projected restricted gamma models
  that we saw in Figures~\ref{fig:simes},\ref{fig:simppl}.  The unrestricted gamma model is penalized much
  more strongly on real data than we saw with our simulation, performing comparably to the probit-normal
  model.  We also see the restricted gamma model with the log-normal prior performing significantly
  better than the restricted gamma model with the gamma prior on the higher dimensional data, a reversal
  of what we saw on the low dimensional data and indeed from what we saw in the simulation study.

That said, there is much we can glean from this table.  The probit-normal model, developed on a mapping
  from $\mathcal{S}_{2}^{d-1}$ to $(-\infty,\infty)^{d-1}$ via probit transformation of spherical coordinates
  was not competitive with the gamma models on real data, similar to what we saw in the simulation study.

We should also note that we did conduct inference on projected gamma models established on
  $\mathcal{S}_{1}^{d-1}$ and $\mathcal{S}_2^{d-1}$.  Posterior predictive loss and energy score criteria
  favored the model established on $\mathcal{S}_{\infty}^{d-1}$.  \bruno{I confess that I don't understand  what ``model established on $\mathcal{S}_{\infty}^{d-1}$'' means. All models ultimately estimate a distribution on $\mathcal{S}_{\infty}^{d-1}$, and there is not way to directly project onto $\mathcal{S}_{\infty}^{d-1}$ because the transformation is not differentiable.} We also established finite mixture
  models for all presented distributions, but after tuning the number of mixture components, their
  model performance was, and could only be \emph{as good} as the DP mixture.

\begin{figure}[h]
  \centering
  \label{fig:knnkl}
  \includegraphics[width = 6in, height = 3in]{./images/knn_kl_divergence_curves}
  \caption{KL Divergence Curves calculated through the KNN-KL Metric, evaluated between the empirical
  dataset and posterior predictive datasets of various models.  The top row corresponds to the
  8-dimensional data, while the bottom row the 47.  All presented models use a Dirichlet process
  prior, the nominal model being the kernel distribution.}
\end{figure}

Interpreting the KL divergence curves in Figure~\ref{fig:knnkl} is more difficult.  As we saw in the
  simulated case in Figure~\ref{fig:simkld}, the general logic of \emph{best} being that which is
  closest to zero is not necessarily in use here.  We interpret a positive value in the KL divergence metric
  as indicating that, for a given point in the given distribution, we must travel \emph{farther} on
  the target distribution to reach the $k$th neighbor as compared to the given distribution.  In this
  regard, we should regard \emph{large} positive values as indicating a particularly poor fit.  In this
  regard, the projected gamma and probit-normal models do not appear to scale well as they performed
  very poorly on the high dimensional data.  For the projected restricted gamma model, the log-normal
  prior appears to scale better than the gamma prior.  This is in keeping with what we observed in
  Table~\ref{tab:dev}, but is a different result than what we observed in the simulation study, where
  we observed that the log-normal prior only tended towards the gamma prior, never performing better.

% EOF
