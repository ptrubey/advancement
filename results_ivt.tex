% results.tex
\subsection{Integrated Vapor Transport}
We use data from the integrated water vapor transport (\emph{IVT}) model\makenote{needs citation}
  of atmospheric rivers as a means of testing these models.  An atmospheric river is a meteorological
  event of local concentration of water vapor in the atmosphere that moves with wind patterns.
  Understanding extremal dependence in atmospheric rivers, for instance, grants hydrological engineers
  a greater understanding of how and where to build ground water capacity to handle precipitation.
  California suffers persistent, extended droughts--if ground water transportation and holding capacity
  (canals, reservoirs) are not built to manage extreme precipitation without adequate understanding of
  the extremal dependence, then we may not install adequate capacity and thus experience flooding.
  Alternatively, if we install too much capacity everywhere, then we are wasting money that could be
  better spent elsewhere.

Fitting our models to this data requires some pre-processing.  The marginal distributions of the
  IVT data appear naturally log-normal, which falls into the domain of attraction of a Gumbel
  distribution.  Given that, we can apply a high threshold, and exceedances over that threshold can
  be modelled as Generalized Pareto.  Our modelling begins with that assessment.  As estimating
  the Pareto parameters is not yet our focus in this analysis, we choose to apply the threshold
  using the empirical CDF.  That is, for a given $t$, let $b_{tl} = \hat{F}_l^{-1}(1 - t^{-1})$.  For
  this analysis, we set $t = 20$, indicating the marginal $95$ percentile.  The other parameters of the
  generalized Pareto--the scale parameter $\alpha_{tl}$ and the extremal index $\chi_l$--are set via
  maximum likelihood.  A fully Bayesian model formulation will allow their varying within a
  distribution, but such a model formulation will not be conducive to fitting by Markov-chain Monte
  Carlo methods.

After the thresholding and maximum likelihood estimation of the parameters of the Pareto, we scale
  the data to the standard multivariate Pareto.  Dividing each standardized observation by its
  $\mathcal{L}_{\infty}$ norm, we project the standardized data onto $\mathcal{S}_{\infty}^{d-1}$.
  Data in sequence represents observations in time, and these are heavily correlated.  As such, we
  choose to \emph{decluster} the observations, such that, by observing a sequence of observations
  $\bm{ z}$ for which $\inorm{z_i} > 1$ for each observation in sequence, we keep only the observation
  with the greatest observed $\mathcal{L}_{\infty}$ norm.  The complete procedure is outlined in
  Algorithm~\ref{algo:processing}.

\begin{algorithm}
  \label{algo:processing}
  \KwResult{$\bm{ r} : r_i \sim \text{Pareto}(1)$, $\bm{ v} \in \mathcal{S}_{\infty}^{d-1}$}
  \For{$l = 1,ldots,d$}{
    Set $b_{tl} = \hat{F}_l^{-1}\left(1 - \frac{1}{t}\right)$.\\
    With $\bm{ x}_l > b_{tl}$, fit $a_{tl}$, $\chi_l$ via MLE according to generalized Pareto likelihood.\\
    }
  \For{$i = 1,\ldots,n$}{
    Define $z_{il} = \left(1 + \xi_l\frac{x_{il} - b_{t,l}}{a_{t,l}}\right)_{+}^{1/\xi_l}$\\
    Define $r_i = \pnorm{\bm{ z}_i}{\infty}$, $\bm{ V}_i = \frac{\bm{ z}_i}{\pnorm{\bm{ z}_i}{\infty}}$\\
    }
  Subset $\bm{ r},\bm{ v}$ such that $r_i \geq 1$ for all $r_i\in \bm{r}$.\\
  \If{declustering}{
    \For{$i = 1,\ldots,n$}{
      If $r_i \geq 1$ and $r_{i-1} \geq 1$, drop the lesser (and associated $v_i$) from dataset.\\
    }
  }
\end{algorithm}

We have, at our disposal, two datasets from the IVT model.  One records data from 8 grid cells, covering
  generally the coast of California.  The other, with a higher resolution, records data from 46 grid
  cells covering the same area.  We fit our models to both datasets.

\begin{table}[h]
  \label{tab:dev}
  \include{table_dev}
  \caption{Model comparison metrics: Posterior Predictive Loss and Energy Score criteria from fitted
    models against the IVT data.  All presented models are DP mixtures; the \emph{Model} field
    identifies the kernel distribution.  For both criteria, lower is better.}
\end{table}

In Table~\ref{tab:dev} we see the same model preference towards the projected restricted gamma models
  that we saw in Figures~\ref{fig:simes,fig:simppl}.  The unrestricted gamma model is penalized much
  more strongly on real data than we saw with our simulation, performing comparably to the probit-normal
  model.  We also see the restricted gamma model with the log-normal prior performing significantly
  better than the restricted gamma model with the gamma prior on the higher dimensional data, a reversal
  of what we saw on the low dimensional data and indeed from what we saw in the simulation study.

That said, there is much we can glean from this table.  The probit-normal model, developed on a mapping
  from $\mathcal{S}_{2}^{d-1}$ to $(-\infty,\infty)^{d-1}$ via probit transformation of spherical coordinates
  was not competitive with the gamma models on real data, similar to what we saw in the simulation study.

We should also note that we did conduct inference on projected gamma models established on
  $\mathcal{S}_{1}^{d-1}$ and $\mathcal{S}_2^{d-1}$.  Posterior predictive loss and energy score criteria
  favored the model established on $\mathcal{S}_{\infty}^{d-1}$.  We also established finite mixture
  models for all presented distributions, but after tuning the number of mixture components, their
  model performance was only \emph{as good} as the DP mixture.

% \begin{figure}[h]
%   \centering
%   \label{fig:knnkl}
%   \includegraphics[width = 5in]{./images/kl_divergence_curves}
%   \caption{KL Divergence Curves calculated through the KNN-KL Metric, evaluated between the empirical
%   dataset and posterior predictive datasets of various models.  The top row corresponds to the
%   8-dimensional data, while the bottom row the 46.  The left column corresponds to the vanilla models
%   with no mixture method; the middle column a Dirichlet process prior, and the right column a finite
%   mixture model.}
% \end{figure}

Interpreting the KL divergence curves in Figure~\ref{fig:knnkl}, KL divergence at its core is a
  log-ratio of densities.  We would prefer, all else being equal, a KL divergence near 0.
  Interpreting this particular divergence metric, a KL divergence less than 0 means on average,
  that the $k$th closest sample from the posterior predictive dataset is closer to the observation
  than the $k$th closest sample from the empirical dataset.  There is a logged ratio of sample sizes
  to account for differences in cardinality between the empirical dataset and posterior predictive.

From this, we would interpret the \emph{best} model as that model which remains closest to the
  horizontal line at $0$.  For the 8-dimension model, this means the generalized Dirichlet and
  projected Gamma models, the unrestricted Gamma based models, seem to perform the best.   This is
  in stark contrast to our inference from the earlier posterior predictive loss and energy score
  criterions, which sharply favored the restricted Gamma models.  However, as dimensionality
  increases, we see that the restricted Gamma models again become favored, indicating that the
  greater flexibility of the unrestricted Gamma models becomes burdensome to fit in a
  high-dimensional case.




% EOF
