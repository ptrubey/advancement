% results.tex
\subsection{Integrated Vapor Transport}
We use data from the integrated water vapor transport (\emph{IVT}) model\makenote{needs citation}
  of atmospheric rivers as a means of testing these models.  An atmospheric river is a meteorological
  event of local concentration of water vapor in the atmosphere that moves with wind patterns.
  Understanding extremal dependence in atmospheric rivers, for instance, grants hydrological engineers
  a greater understanding of how and where to build ground water capacity to handle precipitation.
  California suffers persistent, extended droughts--if ground water transportation and holding capacity
  (canals, reservoirs) are not built to manage extreme precipitation without adequate understanding of
  the extremal dependence, then we may not install adequate capacity and thus experience flooding.
  Alternatively, if we install too much capacity everywhere, then we are wasting money that could be
  better spent elsewhere.

Fitting our models to this data requires some pre-processing.  The marginal distributions of the
  IVT data appear naturally log-normal, which falls into the domain of attraction of a Gumbel
  distribution.  Given that, we can apply a high threshold, and exceedances over that threshold can
  be modelled as Generalized Pareto.  Our modelling begins with that assessment.  As estimating
  the Pareto parameters is not yet our focus in this analysis, we choose to apply the threshold
  using the empirical CDF.  That is, for a given $t$, let $b_{tl} = \hat{F}_l^{-1}(1 - t^{-1})$.  For
  this analysis, we set $t = 20$, indicating the marginal $95$ percentile.  The other parameters of the
  generalized Pareto--the scale parameter $\alpha_{tl}$ and the extremal index $\chi_l$--are set via
  maximum likelihood.  A fully Bayesian model formulation will allow their varying within a
  distribution, but such a model formulation will not be conducive to fitting by Markov-chain Monte
  Carlo methods.

After the thresholding and maximum likelihood estimation of the parameters of the Pareto, we scale
  the data to the standard multivariate Pareto.  Dividing each standardized observation by its
  $\mathcal{L}_{\infty}$ norm, we project the standardized data onto $\mathcal{S}_{\infty}^{d-1}$.
  Data in sequence represents observations in time, and these are heavily correlated.  As such, we
  choose to \emph{decluster} the observations, such that, by observing a sequence of observations
  $\bm{ z}$ for which $\inorm{z_i} > 1$ for each observation in sequence, we keep only the observation
  with the greatest observed $\mathcal{L}_{\infty}$ norm.  The complete procedure is outlined in
  Algorithm~\ref{algo:processing}.

\begin{algorithm}
  \label{algo:processing}
  \KwResult{$\bm{ r} : r_i \sim \text{Pareto}(1)$, $\bm{ v} \in \mathcal{S}_{\infty}^{d-1}$}
  \For{$l = 1,ldots,d$}{
    Set $b_{tl} = \hat{F}_l^{-1}\left(1 - \frac{1}{t}\right)$.\\
    With $\bm{ x}_l > b_{tl}$, fit $a_{tl}$, $\chi_l$ via MLE according to generalized Pareto likelihood.\\
    }
  \For{$i = 1,\ldots,n$}{
    Define $z_{il} = \left(1 + \xi_l\frac{x_{il} - b_{t,l}}{a_{t,l}}\right)_{+}^{1/\xi_l}$\\
    Define $r_i = \pnorm{\bm{ z}_i}{\infty}$, $\bm{ V}_i = \frac{\bm{ z}_i}{\pnorm{\bm{ z}_i}{\infty}}$\\
    }
  Subset $\bm{ r},\bm{ v}$ such that $r_i \geq 1$ for all $r_i\in \bm{r}$.\\
  \If{declustering}{
    \For{$i = 1,\ldots,n$}{
      If $r_i \geq 1$ and $r_{i-1} \geq 1$, drop the lesser (and associated $v_i$) from dataset.\\
    }
  }
\end{algorithm}

Post-standardization, the radial component will follow a standard Pareto distribution.  The
  subject of this paper is a parametric representation of the angular component $\bm{v}$, to which
  we fit the models presented.  We then generate samples from the posterior predictive
  distributions--both overall, for use with the KL divergence metric, and conditioned on individual
  observations, for use with the posterior predictive loss and energy score metrics.

We have, at our disposal, two datasets from the IVT model.  One records data from 8 grid cells,
  covering generally the coast of California.  The other, with a higher resolution, records data
  from 46 grid cells covering the same area.  We fit our models to both datasets.

\begin{table}[h]
  \label{tab:dev}
  \include{table_dev}
  \caption{Model comparison metrics: Posterior Predictive Loss and Energy Score criteria from fitted
    models against the IVT data.  All presented models are DP mixtures; the \emph{Model} field
    identifies the kernel distribution.  For both criteria, lower is better.}
\end{table}

In Table~\ref{tab:dev} we see the same model preference towards the projected restricted gamma models
  that we saw in Figures~\ref{fig:simes,fig:simppl}.  The unrestricted gamma model is penalized much
  more strongly on real data than we saw with our simulation, performing comparably to the probit-normal
  model.  We also see the restricted gamma model with the log-normal prior performing significantly
  better than the restricted gamma model with the gamma prior on the higher dimensional data, a reversal
  of what we saw on the low dimensional data and indeed from what we saw in the simulation study.
  
That said, there is much we can glean from this table.  The normal model, which we developed after
  having cast the data from $S_{\infty}^{d-1}$ to the $d-1$ cube $[0,\pi/2]^{d-1}$ via spherical
  coordinate transformation, then scaled via probit transformation to $(-\infty,\infty)$, was not
  competitive when we transform samples from its posterior predictive distribution back onto
  $S_{\infty}^{d-1}$. The spherical coordinate transformation induces a high degree of dependence
  between dimensions, so it is difficult to think of the geometry of that space as orthogonal.
  Never the less, fitting a normal model in that space assumes something like orthogonality, which
  obviously doesn't hold.  The resulting low model performance in the normal model is likely a
  result of this distortion.

Another inference to learn from this table is the generally better results of the restricted Gamma
  models--Dirichlet, projected restricted Gamma--as compared to the Gamma models that allow for a
  varying rate parameter--generalized Dirichlet and projected Gamma.  Note that regardless of what
  prior was used for the shape parameter, we always assumed a Gamma prior for the rate parameter if
  we allowed it to vary.  In one model formulation, we attempted a multivariate log-normal prior that
  covered both the rate and shape parameters, but this overall resulted in worse model performance
  even comparing to our current results.

It is no surprise that Dirichlet process models are generally preferred as compared to the finite
  mixture models.  Properly tuned, finite mixture models will likely only do \emph{as well} as the
  Dirichlet process mixture model when their number of extant model components are in rough
  agreement.  The Dirichlet process just allows us to bypass to some extent that tuning process.
  That said, for a given performance, the finite mixture model would be preferred, as much of the
  model sampling can be accomplished in parallel.

The final, and most important, revelation this table contains concerns relative model performance
  between the incarnations of the restricted Gamma model.  Given the mixture method, the Dirichlet
  and projected restricted Gamma model are effectively the same model, just built on a different
  projection of the original data.  And indeed, on the 8 dimensional data, the model performance is
  comparable.  But on the higher dimensional data, we see model performance favor the model built on
  $S_{2}^{d-1}$ rather than $S_1^{d-1}$, the simplex.  That difference in model performance is of
  interest.

\begin{figure}[h]
  \centering
  \label{fig:knnkl}
  \includegraphics[width = 5in]{./images/kl_divergence_curves}
  \caption{KL Divergence Curves calculated through the KNN-KL Metric, evaluated between the empirical
  dataset and posterior predictive datasets of various models.  The top row corresponds to the
  8-dimensional data, while the bottom row the 46.  The left column corresponds to the vanilla models
  with no mixture method; the middle column a Dirichlet process prior, and the right column a finite
  mixture model.}
\end{figure}

Interpreting the KL divergence curves in Figure~\ref{fig:knnkl}, KL divergence at its core is a
  log-ratio of densities.  We would prefer, all else being equal, a KL divergence near 0.
  Interpreting this particular divergence metric, a KL divergence less than 0 means on average,
  that the $k$th closest sample from the posterior predictive dataset is closer to the observation
  than the $k$th closest sample from the empirical dataset.  There is a logged ratio of sample sizes
  to account for differences in cardinality between the empirical dataset and posterior predictive.

From this, we would interpret the \emph{best} model as that model which remains closest to the
  horizontal line at $0$.  For the 8-dimension model, this means the generalized Dirichlet and
  projected Gamma models, the unrestricted Gamma based models, seem to perform the best.   This is
  in stark contrast to our inference from the earlier posterior predictive loss and energy score
  criterions, which sharply favored the restricted Gamma models.  However, as dimensionality
  increases, we see that the restricted Gamma models again become favored, indicating that the
  greater flexibility of the unrestricted Gamma models becomes burdensome to fit in a
  high-dimensional case.



% EOF
