\subsection{Projection onto an arbitrary unit hypersphere}
A hypersphere is a geometric object such that the distance from any point to the center takes a fixed,
  constant value.  The unit hypersphere is a hypersphere where that distance is 1. We can define the
  hypersphere under an arbitrary distance measurement, so let's take the $\mathcal{L}_p$ norm. Let
  the $\mathcal{L}_p$-norm be defined as
  \begin{equation*}
    \lVert {\bf s} \rVert_p = \left(\sum_{l = 1}^d \lvert s_l\rvert^p\right)^{\frac{1}{p}}.
  \end{equation*}
  From this, we establish the $\mathcal{L}_1$ norm as $p = 1$, or the absolute sum, equivalently called
  Manhattan distance; the $\mathcal{L}_2$ norm, as $p = 2$, the Euclidean distance.  From this we
  also establish the $\mathcal{L}_{\infty}$ norm, as
  $\lim\limits_{p\to\infty} \lVert {\bf s} \rVert_p = \max_{l\in\lbrace{1,\ldots,d}}s_l$.

We are interested in the direction, or angular distribution, of vectors described in the positive
  orthant, $\mathcal{R}_{+}^d$.  As we are specifically interested in direction, we can project any
  distribution in $\mathcal{R}_{+}^d$ onto the positive orthant of the unit hypersphere in a
  $\mathcal{L}_p$-norm, denoted as $\mathcal{S}_{p}^{d-1}$.  That is,
  \begin{equation*}
    \mathcal{S}_{p}^{d-1} = \left\lbrace {\bf y} : {\bf y} \in \mathcal{R}_{+}^{d}, \lVert {\bf y}\rVert_{p} = 1\right\rbrace.
  \end{equation*}
  We can project an observation onto this space by dividing said observation by its $p$-norm.  That
  is, let ${\bf x}\in \mathcal{R}_{+}^{d}$, then
  ${\bf y} = {\bf x} / \lVert {\bf x}\rVert_p \in \mathcal{S}_{p}^{d-1}$.  We denote the $d-1$
  to indicate the loss of one degree of freedom relative to the original vector.

So $\mathcal{S}_{1}^{d-1}$ defines the unit simplex, $\mathcal{S}_{2}^{d-1}$ defines the generalization of a
  circle--what we would generally refer to as a hypersphere, and $\mathcal{S}_{\infty}^{d-1}$ the surface of
  the hypercube. The hyperspheres defined by $\mathcal{L}_p$ as $p$ varies have a one to one correspondance
  with eachother, meaning that observations on one can be projected onto another without loss of
  information.

Assuming ${\bf y} \in \mathcal{S}_{p}^{d-1}$, \bruno{sometimes you use bold faces to denote vectors and sometimes you don't. You need to be consistent.}then for finite $p$, $y_d$ can always be represented as a
  function of the other dimensions.  That is,
  \begin{equation*}
    y_d = \left(1 - \sum_{l = 1}^{d-1}y_l^p\right)^{\frac{1}{p}}.
  \end{equation*}
  So the transformation
  \begin{equation*}
    T(x_1,\ldots,x_d) = \left(\pnorm{{\bf x}}, \frac{x_1}{\pnorm{{\bf x}}}, \ldots , \frac{x_{d-1}}{\pnorm{{\bf x}}}\right) = (r,y_1,\ldots,y_{d-1})
  \end{equation*}
  does not lose any information.  The reverse of this transformation,
  \begin{equation*}
    T^{-1}\left(r,y_1,\ldots,y_{d-1}\right) =
      \left(ry_1,\ldots,ry_{d-1},r\left(1 - {\scriptstyle\sum}_{l = 1}^{d-1}y_l^p\right)^{\frac{1}{p}}\right)
  \end{equation*}
  equivalently recovers the original data.  The determinant of the Jacobian of this transformation
  takes the form
  \begin{equation*}
    r^{d-1}\left[\left(1 - \sum_{l = 1}^{d-1}y_l^p\right)^{\frac{1}{p}} +
        \sum_{l = 1}^{d-1}y_l^p\left(1 - \sum_{l=1}^d\right)^{\frac{1}{p} - 1}\right].
  \end{equation*}
  \bruno{There is something missing in this formula}
  Notice a factor of $r^{d-1}$ independent of $p$. We refer to $\bm{y}$ and $r$ as, respectively,
  the angular and radial components of $\bm{x}$.}  If we assume a distribution for ${\bf x}$, then
  by transforming to $r, {\bf y}$ and integrating out $r$, we are left with a distribution on solely
  the angular component, or, equivalently, the projection of the vector ${\bf x}$ onto
  $\mathcal{S}_{p}^{d-1}$.

Many of the models we present here follow this form, where we, for reasons to be elaborated, assume
  a $d$-dimensional Gamma distribution on this hypothetical ${\bf x}$. For finite $p$, this has a
  direct benefit in that it is easy to integrate out $r$.  As we saw with the Jacobian computed
  earlier, no matter what $p$, the Jacobian always has a factor of $r^{d-1}$.  With the independent
  Gamma model, $r$ easily integrates out as a gamma distribution.  We can also perform data
  augmentation generating latent $r$'s, and recovering the ability to do independent inference on
  the parameters of those gamma distributions.  We investigated other unidimensional distributions
  with support on $\mathcal{R}_+$ in the hopes we could perform the same dimension reduction with a
  different parameterization, but none offered the flexibility of the Gamma while allowing $r$ to be
  integrated out in closed form.

One might question why we don't use this method to construct a distribution directly on
  $\mathcal{S}_{\infty}^{d-1}$, the unit hypersphere under $\mathcal{L}_{\infty}$.  Put simply, we
  encounter a problem in the transformation.  If we examine the the determinant of the Jacobian
  under the $\mathcal{L}_{p}$ norm, we have a factor along the lines of
  \begin{equation*}
    \left(1 - \sum_{l = 1}^{d-1}y_i^p\right)^{\frac{1}{p} - 1}
  \end{equation*}
  which, if we take the limit as $p$ approaches infinity, if any other $y_l$ than $y_d$ is equal to
  1, then that value approaches $0^{-1}$--an impossibility.  We see a clear breaking point
  between inference conducted on the finite $p$ hypersphere, $\mathcal{S}_{p}^{d-1}$, and the
  $\mathcal{L}_{\infty}$ hypersphere, $\mathcal{S}_{\infty}^{d-1}$. Another way we can recognize
  this issue is from the transformation itself: under $\mathcal{L}_{\infty}$,
  $T^{-1}(r,y_1,\ldots,y_{d-1})$ will not recover $x_1,\ldots,x_d$ if $y_d \neq 1$, or equivalently
  $x_d \neq \max_i x_i$.

With this in mind, the way one might build a distribution on $\mathcal{S}_{\infty}^{d-1}$ that still operates
  in Cartesian coordinates geometry might be to include an equal weighting mixture model, where each
  component of the mixture represents the probability of an observation being on that face, times
  the conditional density of the other dimensions given the face.  That is,
  \begin{equation*}
    f(y) = \sum_{l = 1}^{d}p(y_l = 1)f(y_{-l}\mid y_l = 1)
  \end{equation*}
  Under this interpretation, we can consider $p(y_l = 1) = P(x_l = \max_i x_i)$.  Unfortunately,
  this calculation is not straitforward.

  \makenote{Include arguments from stackexchange thread; cite accordingly}
  \bruno{You need to sharpen the last two paragraphs because I am having a hard time understanding what you are trying to say here.}
  \makenote{I've tried to make it a little more clear.}

Alternatively, one can also map ${\bf y}\in \mathcal{S}_{p}^{d-1}$ into an alternative geometry,
  where we can express those $d-1$ degrees of freedom in a $d-1$ dimensional vector.  On the
  $\mathcal{L}_1$ norm, one might consider isometric or additive logratios\cite{aitchison1982} as
  an appropriate geometry.  On the $\mathcal{L}_2$ norm, we consider spherical coordinates.  This
  directly maps $S_2^{d-1}$ to $[0,\pi/2]^{d-1}$. \cite{nunez2019} follows this course, starting
  with the independent Gamma distribution and still integrating out $r$ to create an angular
  distribution in $[0,\pi/2]^{d-1}$. But we can also construct a distribution directly in this
  space.  Along this idea, via probit transformation we map $[0,\pi/2]^{d-1}$ to
  $(-\infty, \infty)^{d-1}$, and construct a multivariate normal distribution in this geometry.





% EOF
