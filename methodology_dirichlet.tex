\subsection{Dirichlet}
As a distribution defined on the unit hypersphere using the $L_1$ norm, or simplex, Dirichlet is a
  natural choice for our purpose.  The dirichlet random variable can be decomposed as a vector of
  independent gamma random variables with a constant rate parameter, divided by their sum
  (or $L_1$ norm). That is,
  \begin{equation}
    \label{eq:dirichletrv}
    {\bf y} \sim \text{Dir}({\bf y} \mid \zeta) =
        \int_0^{\infty}\prod_{l = 1}^d\text{Ga}(ry_l \mid zeta_l, 1)\lvert J\rvert\text{d}r
  \end{equation}
  \bruno{I don't think I understand this, basically because I don't understand the meaning of $Ga(ry_l| ...)$.}
  The value of the shared rate parameter is irrelevant to the distribution of ${\bf x}$, so by
  convention we set it to one.  The Jacobian for this transformation is $r^{d-1}$. \bruno{Can you give a better explanation of what the idea behind the Dirichlet? I suppose you are mapping $S_\infty^{d-1}$ into $S_1^{d-1}$, right?}

We consider two mixture model approaches under this family; a finite mixture model of fixed
  dimension, and an infinite mixture model using a Dirichlet process prior for $\zeta$.
  We For comparison, we also consider a vanilla Dirichlet model.

\subsubsection{Finite Mixture of Dirichlets}
\label{model:md}
The finite dirichlet mixture model, \emph{MD}, attempts to represent the distribution of data, projected onto
  the simplex, using a finite mixture of Dirichlet parameters $\zeta$.  That is, for each mixture
  component $j$, we have a vector $\zeta_j$ detailing the parameters of the Dirichlet distribution
  under which observations under that component are distributed.
  \begin{equation}
    \label{eq:fmdirichlet}
    \begin{aligned}
      (r_i, x_i) \mid \delta_i &\sim \prod{l = 1}^d\text{Ga}(rx_{il}\mid \zeta_{jl}, 1)\\
            \zeta_{jl} \mid \alpha_l,\beta_l &\sim \text{Ga}(\zeta_{jl}\mid \alpha_l, \beta_l)\\
            \alpha_l &\sim \text{Ga}(\alpha_l \mid 0.5, 0.5)\\
            \beta_l &\sim \text{Ga}(\beta_l \mid 2, 2)\\
            \lambda &\sim \text{Dir}(0.5)
    \end{aligned}
  \end{equation}
  Let $i$ denote indexing over the data set, $j$ denote indexing over mixture components, and
  $l$ denote indexing over columns. Thus, $\delta_i$ denotes the mixture component associated
  with the $i$'th observation.  $\zeta_{jl}$ denotes the shape parameter of the Dirichlet
  distribution associated with mixture component $j$, and column $l$.  The purpose of the rate
  parameter hyperpriors being being somewhat informative, is to ensure for numerical stability
  reasons that the rate parameters do not approach 0.

We perform data augmentation, generating $r_i$, and recovering the original product of independent
  gammas interpretation of the Dirichlet RV.  This enables us to do posterior learning about
  $\zeta_{j1}$ independent of $\zeta_{j2}$ within the full conditional.  The augmented variable,
  $r_i$, can be generated as
  \begin{equation}
    \label{eq:L1augmentation}
    r_i\mid x, \delta, \zeta \sim \text{Ga}(r_i \mid \sum_{l = 1}^d \zeta_{jl}, 1)
  \end{equation}
  We assign a prior distribution for probability of mixture component membership, $\lambda$, as a
  Dirichlet RV with a relatively weak 0.5 symmetric shape parameter.

The full conditional distribution for $\zeta_{jl}$ is not available in a known form, so sampling
  will require some flavor of MCMC.  We employ a Metropolis-Hastings sampler on $\log\zeta_{jl}$
  using a normal proposal distribution with a standard deviation of 0.3.  This is also employed in
  the posterior sampling for $\alpha_l$. The full conditional for $\beta_l$ arrives in known form
  as a Gamma,
  \begin{equation}
    \label{eq:betafc}
    \beta_l\mid \alpha_l, \zeta \sim \text{Ga}(\beta_l \mid J\alpha + 2, \sum_{j = 1}^J\zeta_{jl} + 2)
  \end{equation}
  The full conditionals for $\zeta_{jl}$ and $\alpha_l$ \makenote{are yet to be inserted, but they
  are simple gamma/gamma models.}

We construct the finite mixture again using data augmentation, introducing the mixture component
  identifier $\delta_i$. The posterior probability that $\delta_i = j$ is constructed as:
  \begin{equation}
    \label{eq:finitemix}
    p(\delta_i = j \mid r, x, \pi, \zeta) \propto \pi_j\prod_{l = 1}^d\text{Ga}(rx_{il}\mid\zeta_{jl},1)
  \end{equation}
  Then the posterior distribution for $\pi$ is formed from the cluster membership identifiers.  Let
  $n_j = \sum 1_{\delta_i = j}$, then $\pi$ is distributed as
  \begin{equation}
    \pi \mid \delta \sim \text{Dir}(n_1 + 0.5, \ldots, n_J + 0.5).
  \end{equation}
  This comprises the simplest model we present for comparison.

\subsubsection{Dirichlet Process Mixture of Dirichlets}
\label{model:dpd}
The natural extension to the finite mixture of Dirichlets would be to assume the cluster parameters,
  $\zeta_j$, as descending from an infinite mixture.  As we are simply attempting to represent the
  data, and do not have a compelling interest in controlling the number of clusters, a natural
  choice of prior is the Dirichlet process.  We denote this model as \emph{DPD}.  This model will
  share a great deal of construction, posterior inference, and indeed code, with the finite
  mixture model.
  \begin{equation}
    \label{eq:dpsimplex}
    \begin{aligned}
      (r_i, {\bf x}_i) \mid \zeta_i &\sim \prod_{l = 1}^d\text{Ga}(r_ix_{il}\mid \zeta_{il}, 1)\\
      \zeta_i &\sim \text{DP}(\eta, G)\hspace{2cm}G = \prod_{l = 1}^d \text{Ga}(\zeta_{il}\mid \alpha_l,\beta_l)\\
      \alpha_l &\sim \text{Ga}(\alpha_l \mid 0.5, 0.5)\\
      \beta_l &\sim \text{Ga}(\beta_l \mid 2, 2)\\
      \eta &\sim \text{Ga}(\eta \mid 2, \kappa) \hspace{2cm}\kappa \in \lbrace 0.1, 1, 10\rbrace
    \end{aligned}
  \end{equation}

We denote cluster membership using $\delta_i$, as in the previous case.  Let
  $n_j = \sum 1_{\delta_i = j}$ denote the cluster size.  In the DP literature terminology, we
  are using what is referred to as the collapsed sampler, where for existing clusters,
  \begin{equation}
    p(\delta_i = j \mid r, \zeta, \eta) \propto \frac{n_j}{\sum_j n_j + \eta}
                \prod_{l = 1}^d\text{Ga}(r_ix_{il}\mid\zeta_{jl}).
  \end{equation}
For new clusters, ostensibly we would integrate out the cluster parameters to get a true
  posterior predictive density.  However, doing so in this case is not straitforward.  Instead,
  we employ algorithm 8 from \cite{neal2000}, which, instead of evaluating the posterior predictive
  density as a single point, we generate $m$ new candidate clusters given $\alpha,\beta$, then the
  probability of $x_i$ belonging to any particular candidate cluster is given as:
  \begin{equation}
    p(\delta_i = j \mid r, \zeta^{\prime}, \eta) \propto \frac{\eta / m}{\sum_j n_j + \eta}
                \prod_{l = 1}^d\text{Ga}(r_ix_{il}\mid\zeta_{jl}^{\prime})
  \end{equation}
  where $\zeta_j^{\prime}$ indicates the cluster parameters from a candidate cluster.  If a new
  cluster is selected, then we append the new cluster parameters to the stack, and continue to the
  next observation.

This model is slightly more complex than the finite mixture of Dirichlets, but at its core it
  employs the same assumption--that the individual columns descend from independent gamma random
  variables, with a fixed rate parameter.

\subsubsection{Dirichlets with log-normal Prior}
A derivative model we might try is placing a lognormal prior on $\zeta$.  That is, to say, instead
  of having $\zeta$ descend from $d$ independent Gamma random variables, we can have $\zeta$
  descend from a $d$-dimensional log-normal random variable.  The impetus for this variation comes
  from our implementation of the DP mixture model--that we need to generate candidate clusters for
  the shape parameters.  In the previous model, we are generating these clusters by independently
  drawing from $d$ gamma distributions.  Some information in terms of covariance between dimensions
  may be available, and would be lost assuming an independent gamma prior, so a log-normal prior on
  $\zeta$ may serve to capture that information.  That is, for the finite mixture model,
  \begin{equation}
    \label{model:mgdln}
    \begin{aligned}
      (r_i, x_i) \mid \delta_i &\sim \prod_{l = 1}^d\text{Ga}(rx_{il}\mid \zeta_{jl}, 1)\\
        \zeta_{j} \mid \mu, \Sigma &\sim \mathcal{LN}_d\left(\zeta_j\mid\mu,\Sigma\right)\\
        \mu &\sim \mathcal{N}_d\left(\mu\mid\mu_0,\Sigma_0\right)\\
        \Sigma &\sim \text{IW}\left(\Sigma\mid\nu,\Psi\right) \\
        \lambda &\sim \text{Dir}(0.5)
    \end{aligned}
  \end{equation}
  The hope is this will allow us to better capture the relationships between dimensions, and thus
  more efficiently generate candidate clusters for the DP sampler.  The downside, however, is that
  this introduces a $d$-dimensional normal distribution into the model, and for that we suffer the
  the computational complexity that induces.

The DP equivalent of this model has $\zeta_i$ as descending from a Dirichlet Process, with a
  log-normal kernel distribution.  We place a gamma prior on the concentration parameter $\eta$,
  and thereafter the hyperpriors are the same as for the finite mixture model.

\bruno{The three models that you consider in this section are essentially the same model with three variations. You should write the section by presenting a model that consists of a mixture of Dirichlet with two options: a finite mixture and a DP mixture, the a submodel for the DP misture where you consider two choices fot he hyperparameter priors. Now, is it really worth presenting all three different models?}
% EOF
